<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogposts on Yiding&#39;s blog</title>
    <link>https://yidingjiang.github.io/blog/</link>
    <description>Recent content in Blogposts on Yiding&#39;s blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 May 2025 20:29:38 +0800</lastBuildDate>
    <atom:link href="https://yidingjiang.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A compression perspective on curriculum learning</title>
      <link>https://yidingjiang.github.io/blog/post/curriculum/</link>
      <pubDate>Tue, 06 May 2025 20:29:38 +0800</pubDate>
      <guid>https://yidingjiang.github.io/blog/post/curriculum/</guid>
      <description>&lt;p&gt;A fundamental property of an intelligent system is the ability to predict, be it predicting the label of an image, the optimal action to take, or the next token in a sentence. Having a good predictive model is intimately tied to having a good compression algorithm. Conversely, a strong compression algorithm must have a good model that can identify and leverage patterns in the data.&lt;/p&gt;&#xA;&lt;p&gt;While the relationship between compression and intelligence has been thoroughly discussed&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, we will focus here on a particularly interesting relationship between compression and massively overparameterized models, which also naturally lends itself to an objective for data selection and curriculum learning.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
