<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogposts on Yiding&#39;s blog</title>
    <link>https://yidingjiang.github.io/blog/</link>
    <description>Recent content in Blogposts on Yiding&#39;s blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Jun 2025 17:57:46 -0400</lastBuildDate>
    <atom:link href="https://yidingjiang.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Era of Exploration</title>
      <link>https://yidingjiang.github.io/blog/post/exploration/</link>
      <pubDate>Wed, 25 Jun 2025 17:57:46 -0400</pubDate>
      <guid>https://yidingjiang.github.io/blog/post/exploration/</guid>
      <description>&lt;p&gt;Large language models are the unintended byproduct of about three decades worth of freely accessible human text online. Ilya Sutskever compared this reservoir of information to &lt;a href=&#34;https://www.youtube.com/watch?v=YD-9NG1Ke5Y&#34;&gt;fossil fuel&lt;/a&gt;, abundant but ultimately finite. Some studies suggest that, at current token‑consumption rates, frontier labs could exhaust the highest‑quality English web text well before the decade ends. Even if those projections prove overly pessimistic, one fact is clear: today’s models consume data far faster than humans can produce it.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A compression perspective on curriculum learning</title>
      <link>https://yidingjiang.github.io/blog/post/curriculum/</link>
      <pubDate>Tue, 06 May 2025 20:29:38 +0800</pubDate>
      <guid>https://yidingjiang.github.io/blog/post/curriculum/</guid>
      <description>&lt;p&gt;A fundamental property of an intelligent system is the ability to predict, be it predicting the label of an image, the optimal action to take, or the next token in a sentence. Having a good predictive model is intimately tied to having a good compression algorithm. Conversely, a strong compression algorithm must have a good model that can identify and leverage patterns in the data.&lt;/p&gt;&#xA;&lt;p&gt;While the relationship between compression and intelligence has been thoroughly discussed&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, we will focus here on a particularly interesting relationship between compression and massively overparameterized models, which also naturally lends itself to an objective for data selection and curriculum learning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adaptive Data Optimization</title>
      <link>https://yidingjiang.github.io/blog/post/ado/</link>
      <pubDate>Sun, 13 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://yidingjiang.github.io/blog/post/ado/</guid>
      <description></description>
    </item>
  </channel>
</rss>
